## EC2 네트워크 최적화

- EC2 네트워크 최적화는 한 가지 해결방안으로는 최적화되지 않는다
- 여러 관점에서 최적화가 이루어져야 한다
- AWS 는 아래 4개의 레이어에서 최적화를 제공한다
  - Placement Group → 물리적인 노드 배치
  - EBS-Optimized → 스토리지 경로
  - Enhanced Networking → NIC
  - DPDK → 패킷 처리 방식
- 어디가 병목인지에 따라 선택이 달라진다

</br>

### Placement Groups - Cluster

- EC2 인스턴스들을 단일 AZ 안에서 물리적으로 최대한 가깝게 배치하는 기능
  - 같은 랙 또는 매우 가까운 네트워크 패브릭에 배치
- 왜 네트워크가 빨라지는가?
  - 물리적인 거리가 줄어듦
  - 스위치 홉 개수가 줄어듦
  - 지연이 낮아지며 대역폭이 높아짐
- 단점
  - 단일 AZ 기준으로만 적용가능 → 멀티 AZ 가용성은 포기
  - 인스턴스 타입의 제한이 존재함 → c5n, r5n 등
  - 물리 용량 부족 시 인스턴스 생성 실패할 가능성이 존재함
- 가용성보다 네트워크 성능이 더 중요한 워크로드인 경우 고려할 수 있는 방안

</br>

### EBS-Optimized Instances

- EC2 <-> EBS 사이에 전용 네트워크 대역폭을 제공한다
- 일반 네트워크 트래픽과 스토리지 트래픽을 분리하여 관리한다
- EBS 는 네트워크 기반 스토리지 워크로드이며 네트워크가 병목이 생긴다면 디스크 성능도 같이 떨어지게 된다
- 이를 EBS-Optimized 하여 네트워크 혼잡 방지 및 IOPS, 처리량 안정성을 확보한다
- IO 성능 예측 가능성이 중요한 경우 고려할만 하다
- 요즘 대부분의 최신 인스턴스는 기본이 활성화되어 있다
- 하지만 인스턴스 타입별 최대 IOPS/MBps 한도는 여전히 존재한다
  - gp3/io2 타입 설정이 병목일 수도 있다

</br>

### Enhanced Networking

- 하이퍼바이저를 우회하거나 최소화하여 네트워크 성능을 끌어올린다는 개념이다

**SR-IOV**

- 하나의 물리 NIC 를 여러 VM 이 하드웨어 레벨에서 직접적으로 공유하는 방식
- 하이퍼바이저를 거치지 않고 직접 공유
- CPU 오버헤드가 감소하며 지연시간이 감소함
- 대부분의 기본 Enhanced Networking 은 이 구조를 차용함

**ENA (Elastic Network Adapter)**

- AWS 전용 고성능 네트워크 어댑터
- 내부적으로 SR-IOV 기반이며 최대 100Gbps 성능이 나옴
- 최근 EC2 의 기본 네트워크 카드 → C5/M5/R5 이후의 EC2 타입 기준
- 점보 프레임을 지원함 → MTU 9001
- 고성능 및 범용성의 균형을 가짐

**EFA (Elastic Fabric Adapter)**

- HPC, ML 전용 네트워크 구축
- TCP/IP 스택 자체를 우회하는 방식
- RDMA 기반 통신 구축
- 최대 400Gbps 성능이 나오며 분산 학습에 최적화된 방식

</br>

### DPDK

- Linux 기반 운영체제에서 Linux 커널 네트워크 스택을 완전히 우회함
- 유저 공간에서 패킷을 직접 처리하는 방식
- 커널 인터럽트가 없으며 컨텍스트 스위칭을 최소화한다, 초당 수백만 PPS 처리 가능
- 초고속 패킷 처리 시스템이나 NFV (방화벽, 라우터) 에서 사용함
- 그러나 개발 난이도가 높으며 커널 기능(TCP 스택)을 직접 구현할 필요가 있음

</br>

## 네트워크는 빠른데 응답이 느린 경우의 사례 분석

- 네트워크가 빠르다는 건 전송 구간이 빠르다는 뜻이지 응답 전체가 빠르다는 개념은 아니다 
- 이 경우 App Thread, Event Loop 구간 혹은 DB/Cache/외부 API 부분이 문제일 가능성이 있다 

**네트워크 지표는 좋지만 p95가 느린경우**

- 애플리케이션 스레드 고갈 원인 파악 필요 
    - 네트워크를 통해 데이터 도착 → 요청은 들어왔는데 처리할 스레드가 없어 큐에서 대기하는 상황 
- Tomcat worker thread 부족, Coroutine Dispatcer 제한, DB 커넥션 풀 대기 등의 패턴이 나올 수 있음 

**대역폭은 남지만 느리다**

- 10Gbps 이상 여유가 되며 패킷 드롭도 없지만 CPU 사용률이 70% ~ 80% 를 달성하려고 한다면 
- PPS (Packet Per Second) 병목 원인 파악 필요 
    - 작은 패킷이 너무 많거나 JSON/gRPC 메시지 다량 혹은 TCP ACK 처리 과부화 문제일 수 있음 
    - 이 경우 MTU (점보 프레임), PPS 감소 전략이 의미있음 

**ALB 이후에는 빠르지만 사용자 체감이 느리다**

- ALB → Target 간의 keep-alive 설정이 문제 
    - ALB 는 기본적으로 Target(EC2/ECS/Pod) 과 HTTP 연결을 재사용(keep-alive) 한다 
    - 그런데 다음 상황이라면 매 요청마다 새로운 TCP 연결이 생성된다 
        - Target 서버에서 keep-alive 비활성화 / 서버 idle timeout < ALB idle timeout / 프레임워크 기본 설정 이슈 
    - keep-alive 설정이 정상 동작한다면 TCP 의 handshake 과정이 스킵되며 RTT 최적화가 가능함 
- Cold connection 재사용 안됨 
    - Cold connection 은 기존에 열려 있던 연결이 없어서 새 연결을 만드는 상태를 의미한다 
    - HTTP client connection pool 크기 부족 / pool 크기는 있는데 idle timeout 이 너무 짧음 / 요청이 폭증하는 경우 등에서 발생할 수 있음 
    - 이 경우 네트워크 그래프는 정상일 수 있으나 커넥션을 생성하는 비용이 있으므로 CPU, GC, thread 대기에서 시간이 소비되는 패턴이 발생함 
- 외부 API 
    - 제어할 수 없는 네트워크이며 상대 서버 상태 또한 알지못함 
    - 자신의 네트워크 구간과 외부 API 를 요청한 네트워크 구간을 구분하여 모니터링할 필요가 있음 
